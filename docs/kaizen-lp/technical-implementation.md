# KAIZEN LP - æŠ€è¡“å®Ÿè£…è¨ˆç”»æ›¸

## 1. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å®Ÿè£…ã®è©³ç´°è¨­è¨ˆ

### ğŸ“ è»½é‡ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ5KBä»¥ä¸‹ï¼‰

```javascript
// kaizen-tracker.js - è¶…è»½é‡ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
(function() {
  'use strict';
  
  const KaizenTracker = {
    // è¨­å®š
    config: {
      endpoint: 'https://api.kaizen-lp.com/v1/events',
      sampleRate: 0.1, // 10%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
      batchSize: 20,   // 20ã‚¤ãƒ™ãƒ³ãƒˆã”ã¨ã«é€ä¿¡
      debounceMs: 500  // éå‰°ãªã‚¤ãƒ™ãƒ³ãƒˆé˜²æ­¢
    },
    
    // ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ•ã‚¡
    buffer: [],
    
    // åˆæœŸåŒ–
    init: function(siteId, options) {
      this.siteId = siteId;
      Object.assign(this.config, options);
      
      // ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼è¨­å®šï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
      this.attachListeners();
      
      // ãƒšãƒ¼ã‚¸é›¢è„±æ™‚ã«ãƒãƒƒãƒ•ã‚¡é€ä¿¡
      window.addEventListener('beforeunload', () => this.flush());
      
      // å®šæœŸé€ä¿¡ï¼ˆ30ç§’ã”ã¨ï¼‰
      setInterval(() => this.flush(), 30000);
    },
    
    // ã‚¯ãƒªãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
    trackClick: function(e) {
      if (Math.random() > this.config.sampleRate) return;
      
      const data = {
        type: 'click',
        x: e.pageX,
        y: e.pageY,
        target: e.target.tagName,
        viewport: {
          w: window.innerWidth,
          h: window.innerHeight
        },
        timestamp: Date.now()
      };
      
      this.addToBuffer(data);
    },
    
    // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
    trackScroll: throttle(function() {
      const scrollPercentage = (window.scrollY / 
        (document.documentElement.scrollHeight - window.innerHeight)) * 100;
      
      this.addToBuffer({
        type: 'scroll',
        depth: Math.round(scrollPercentage),
        timestamp: Date.now()
      });
    }, 1000),
    
    // ãƒã‚¦ã‚¹ç§»å‹•ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ï¼‰
    trackMouseMove: throttle(function(e) {
      // 100ãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
      const gridSize = 100;
      const x = Math.floor(e.pageX / gridSize) * gridSize;
      const y = Math.floor(e.pageY / gridSize) * gridSize;
      
      this.addToBuffer({
        type: 'hover',
        x: x,
        y: y,
        duration: 100 // æ»åœ¨æ™‚é–“
      });
    }, 100),
    
    // ãƒ¬ã‚¤ã‚¸ã‚¯ãƒªãƒƒã‚¯æ¤œå‡º
    detectRageClick: function(e) {
      const now = Date.now();
      if (this.lastClick && (now - this.lastClick) < 500) {
        this.rageClickCount = (this.rageClickCount || 0) + 1;
        
        if (this.rageClickCount >= 3) {
          this.addToBuffer({
            type: 'rage_click',
            x: e.pageX,
            y: e.pageY,
            count: this.rageClickCount
          });
        }
      } else {
        this.rageClickCount = 0;
      }
      this.lastClick = now;
    },
    
    // ãƒãƒƒãƒ•ã‚¡ç®¡ç†
    addToBuffer: function(data) {
      this.buffer.push({
        ...data,
        url: window.location.href,
        sessionId: this.getSessionId(),
        deviceType: this.getDeviceType()
      });
      
      if (this.buffer.length >= this.config.batchSize) {
        this.flush();
      }
    },
    
    // ãƒ‡ãƒ¼ã‚¿é€ä¿¡ï¼ˆBeacon APIä½¿ç”¨ï¼‰
    flush: function() {
      if (this.buffer.length === 0) return;
      
      const data = {
        siteId: this.siteId,
        events: this.buffer.splice(0)
      };
      
      // Beacon APIã§ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰é€ä¿¡
      if (navigator.sendBeacon) {
        navigator.sendBeacon(this.config.endpoint, JSON.stringify(data));
      } else {
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        fetch(this.config.endpoint, {
          method: 'POST',
          body: JSON.stringify(data),
          keepalive: true
        });
      }
    }
  };
  
  // ã‚°ãƒ­ãƒ¼ãƒãƒ«å…¬é–‹
  window.KaizenTracker = KaizenTracker;
})();

// ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
function throttle(func, wait) {
  let timeout;
  return function executedFunction(...args) {
    const later = () => {
      clearTimeout(timeout);
      func(...args);
    };
    clearTimeout(timeout);
    timeout = setTimeout(later, wait);
  };
}
```

### ğŸ—„ï¸ ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿å‡¦ç†

```python
# event_processor.py - ã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
import asyncio
from datetime import datetime, timedelta
import json
import numpy as np
from fastapi import FastAPI, BackgroundTasks
from clickhouse_driver import Client
import redis.asyncio as redis

app = FastAPI()
clickhouse = Client('localhost')
redis_client = redis.Redis()

class EventProcessor:
    """é«˜é€Ÿã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.batch_queue = asyncio.Queue()
        self.processing = False
    
    async def process_events(self, events: list, site_id: str):
        """ã‚¤ãƒ™ãƒ³ãƒˆã®éåŒæœŸå‡¦ç†"""
        
        # 1. ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        validated_events = await self.validate_events(events)
        
        # 2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆï¼ˆRedisï¼‰
        await self.update_realtime_metrics(validated_events, site_id)
        
        # 3. ãƒãƒƒãƒå‡¦ç†ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        await self.batch_queue.put({
            'site_id': site_id,
            'events': validated_events,
            'timestamp': datetime.utcnow()
        })
        
        # 4. ãƒãƒƒãƒå‡¦ç†é–‹å§‹ï¼ˆéãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
        if not self.processing:
            asyncio.create_task(self.process_batch())
    
    async def update_realtime_metrics(self, events, site_id):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""
        pipe = redis_client.pipeline()
        
        for event in events:
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            if event['type'] == 'click':
                grid_key = f"heatmap:{site_id}:{event['url']}:{event['x']//10}:{event['y']//10}"
                pipe.hincrby(grid_key, 'clicks', 1)
            
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«æ·±åº¦æ›´æ–°
            elif event['type'] == 'scroll':
                scroll_key = f"scroll:{site_id}:{event['url']}"
                pipe.zadd(scroll_key, {event['sessionId']: event['depth']})
            
            # ãƒ¬ã‚¤ã‚¸ã‚¯ãƒªãƒƒã‚¯è¨˜éŒ²
            elif event['type'] == 'rage_click':
                rage_key = f"rage:{site_id}:{datetime.utcnow().strftime('%Y%m%d')}"
                pipe.hincrby(rage_key, f"{event['x']}:{event['y']}", 1)
        
        await pipe.execute()
    
    async def process_batch(self):
        """ãƒãƒƒãƒå‡¦ç†ï¼ˆClickHouseä¿å­˜ï¼‰"""
        self.processing = True
        batch_data = []
        
        while True:
            try:
                # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¾ã§å¾…æ©Ÿï¼ˆæœ€å¤§1000ä»¶ã¾ãŸã¯5ç§’ï¼‰
                timeout = 5
                start_time = asyncio.get_event_loop().time()
                
                while len(batch_data) < 1000:
                    remaining_time = timeout - (asyncio.get_event_loop().time() - start_time)
                    if remaining_time <= 0:
                        break
                    
                    try:
                        item = await asyncio.wait_for(
                            self.batch_queue.get(), 
                            timeout=remaining_time
                        )
                        batch_data.extend(item['events'])
                    except asyncio.TimeoutError:
                        break
                
                if batch_data:
                    # ClickHouseã«ä¸€æ‹¬æŒ¿å…¥
                    await self.save_to_clickhouse(batch_data)
                    batch_data = []
                    
            except Exception as e:
                print(f"Batch processing error: {e}")
                await asyncio.sleep(1)
    
    async def save_to_clickhouse(self, events):
        """ClickHouseã¸ã®é«˜é€Ÿä¿å­˜"""
        # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        rows = []
        for event in events:
            rows.append((
                event.get('site_id'),
                event.get('session_id'),
                event.get('type'),
                event.get('url'),
                event.get('x', 0),
                event.get('y', 0),
                event.get('timestamp'),
                json.dumps(event.get('metadata', {}))
            ))
        
        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
        clickhouse.execute(
            '''INSERT INTO events 
            (site_id, session_id, type, url, x, y, timestamp, metadata) 
            VALUES''',
            rows
        )

# FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/v1/events")
async def receive_events(
    data: dict, 
    background_tasks: BackgroundTasks
):
    """ã‚¤ãƒ™ãƒ³ãƒˆå—ä¿¡ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    processor = EventProcessor()
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å‡¦ç†
    background_tasks.add_task(
        processor.process_events,
        data['events'],
        data['siteId']
    )
    
    return {"status": "accepted"}
```

## 2. GA4/ã‚µãƒ¼ãƒã‚³ãƒ³ã‚½ãƒ¼ãƒ«é€£æº

### ğŸ”— APIçµ±åˆå®Ÿè£…

```typescript
// analytics-integration.ts
import { google } from 'googleapis';
import { BetaAnalyticsDataClient } from '@google-analytics/data';

class AnalyticsIntegration {
  private ga4Client: BetaAnalyticsDataClient;
  private searchConsoleClient: any;
  
  constructor(credentials: ServiceAccountCredentials) {
    // GA4ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    this.ga4Client = new BetaAnalyticsDataClient({
      credentials
    });
    
    // Search Console APIåˆæœŸåŒ–
    const auth = new google.auth.GoogleAuth({
      credentials,
      scopes: ['https://www.googleapis.com/auth/webmasters.readonly']
    });
    
    this.searchConsoleClient = google.searchconsole({
      version: 'v1',
      auth
    });
  }
  
  async getGA4Data(propertyId: string, dateRange: DateRange) {
    // ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ»ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å–å¾—
    const [response] = await this.ga4Client.runReport({
      property: `properties/${propertyId}`,
      dateRanges: [{
        startDate: dateRange.start,
        endDate: dateRange.end
      }],
      dimensions: [
        { name: 'sessionSource' },
        { name: 'sessionMedium' },
        { name: 'landingPage' },
        { name: 'deviceCategory' }
      ],
      metrics: [
        { name: 'sessions' },
        { name: 'conversions' },
        { name: 'bounceRate' },
        { name: 'averageSessionDuration' }
      ]
    });
    
    return this.transformGA4Response(response);
  }
  
  async getSearchConsoleData(siteUrl: string, dateRange: DateRange) {
    // æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—
    const response = await this.searchConsoleClient.searchanalytics.query({
      siteUrl,
      requestBody: {
        startDate: dateRange.start,
        endDate: dateRange.end,
        dimensions: ['query', 'page', 'device'],
        rowLimit: 1000
      }
    });
    
    return this.transformSearchConsoleResponse(response.data);
  }
  
  async getCrossAnalysis(ga4Data: any, searchData: any) {
    /**
     * GA4ã¨ã‚µãƒ¼ãƒã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆåˆ†æ
     */
    const analysis = {
      // ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â†’ ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®é–¢é€£ä»˜ã‘
      keywordPerformance: this.analyzeKeywordConversion(ga4Data, searchData),
      
      // ãƒ‡ãƒã‚¤ã‚¹åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
      deviceAnalysis: this.analyzeDevicePerformance(ga4Data, searchData),
      
      // ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸æœ€é©åŒ–ææ¡ˆ
      pageOptimization: this.generatePageOptimizations(ga4Data, searchData),
      
      // æµå…¥æ”¹å–„ææ¡ˆ
      trafficRecommendations: this.generateTrafficRecommendations(searchData)
    };
    
    return analysis;
  }
  
  private analyzeKeywordConversion(ga4Data: any, searchData: any) {
    // ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç›¸é–¢åˆ†æ
    const keywordMap = new Map();
    
    searchData.rows.forEach(row => {
      const keyword = row.keys[0];
      const page = row.keys[1];
      
      // GA4ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒƒãƒãƒ³ã‚°
      const pageData = ga4Data.find(d => d.landingPage === page);
      if (pageData) {
        keywordMap.set(keyword, {
          impressions: row.impressions,
          clicks: row.clicks,
          ctr: row.ctr,
          position: row.position,
          conversions: pageData.conversions,
          conversionRate: pageData.conversions / row.clicks
        });
      }
    });
    
    // é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    return Array.from(keywordMap.entries())
      .sort((a, b) => b[1].conversionRate - a[1].conversionRate)
      .slice(0, 20);
  }
}
```

## 3. AIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³

### ğŸ¤– æ”¹å–„ææ¡ˆè‡ªå‹•ç”Ÿæˆ

```python
# ai_analyzer.py
import openai
from typing import List, Dict, Any
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor

class AIAnalyzer:
    """AIé§†å‹•ã®æ”¹å–„ææ¡ˆã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, api_key: str):
        openai.api_key = api_key
        self.model = "gpt-4-turbo-preview"
        
    async def analyze_heatmap(self, heatmap_data: Dict) -> List[Dict]:
        """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‹ã‚‰æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        
        # å•é¡Œç®‡æ‰€ã®æ¤œå‡º
        problems = self.detect_problems(heatmap_data)
        
        # å„å•é¡Œã«å¯¾ã™ã‚‹æ”¹å–„ææ¡ˆç”Ÿæˆ
        suggestions = []
        for problem in problems:
            suggestion = await self.generate_suggestion(problem)
            suggestions.append(suggestion)
        
        # å„ªå…ˆé †ä½ä»˜ã‘
        prioritized = self.prioritize_suggestions(suggestions)
        
        return prioritized
    
    def detect_problems(self, heatmap_data: Dict) -> List[Dict]:
        """å•é¡Œç®‡æ‰€ã®è‡ªå‹•æ¤œå‡º"""
        problems = []
        
        # CTAã‚¯ãƒªãƒƒã‚¯ç‡ãŒä½ã„
        if heatmap_data['cta_click_rate'] < 0.02:  # 2%æœªæº€
            problems.append({
                'type': 'low_cta_engagement',
                'severity': 'high',
                'metrics': {
                    'current_rate': heatmap_data['cta_click_rate'],
                    'benchmark': 0.05,
                    'location': heatmap_data['cta_position']
                }
            })
        
        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ç‡ãŒä½ã„
        if heatmap_data['scroll_50_percent'] < 0.5:  # 50%åœ°ç‚¹åˆ°é”ãŒ50%æœªæº€
            problems.append({
                'type': 'high_bounce_rate',
                'severity': 'medium',
                'metrics': {
                    'scroll_depth': heatmap_data['avg_scroll_depth'],
                    'bounce_point': heatmap_data['common_exit_point']
                }
            })
        
        # ãƒ¬ã‚¤ã‚¸ã‚¯ãƒªãƒƒã‚¯ç™ºç”Ÿ
        if heatmap_data['rage_clicks'] > 10:
            problems.append({
                'type': 'user_frustration',
                'severity': 'high',
                'metrics': {
                    'rage_click_areas': heatmap_data['rage_click_coordinates'],
                    'frequency': heatmap_data['rage_clicks']
                }
            })
        
        return problems
    
    async def generate_suggestion(self, problem: Dict) -> Dict:
        """GPT-4ã‚’ä½¿ã£ãŸæ”¹å–„ææ¡ˆç”Ÿæˆ"""
        
        prompt = f"""
        LPã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä»¥ä¸‹ã®å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼š
        
        å•é¡Œã‚¿ã‚¤ãƒ—: {problem['type']}
        æ·±åˆ»åº¦: {problem['severity']}
        ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {problem['metrics']}
        
        ã“ã®å•é¡Œã«å¯¾ã™ã‚‹å…·ä½“çš„ãªæ”¹å–„ææ¡ˆã‚’3ã¤æä¾›ã—ã¦ãã ã•ã„ã€‚
        å„ææ¡ˆã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
        1. å…·ä½“çš„ãªå®Ÿè£…æ–¹æ³•
        2. æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœï¼ˆæ•°å€¤ï¼‰
        3. å®Ÿè£…ã®é›£æ˜“åº¦ï¼ˆä½/ä¸­/é«˜ï¼‰
        4. å®Ÿè£…ã«å¿…è¦ãªæ™‚é–“
        
        JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        """
        
        response = await openai.ChatCompletion.acreate(
            model=self.model,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯LPOå°‚é–€å®¶ã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        suggestion = json.loads(response.choices[0].message.content)
        suggestion['problem'] = problem
        
        return suggestion
    
    def prioritize_suggestions(self, suggestions: List[Dict]) -> List[Dict]:
        """æ”¹å–„ææ¡ˆã®å„ªå…ˆé †ä½ä»˜ã‘"""
        
        for suggestion in suggestions:
            # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå½±éŸ¿åº¦ Ã— å®Ÿè£…å®¹æ˜“æ€§ï¼‰
            impact_score = suggestion.get('expected_impact', 0.5)
            ease_score = {
                'ä½': 1.0,
                'ä¸­': 0.6,
                'é«˜': 0.3
            }.get(suggestion.get('difficulty', 'ä¸­'), 0.5)
            
            suggestion['priority_score'] = impact_score * ease_score
        
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        return sorted(suggestions, key=lambda x: x['priority_score'], reverse=True)

class MLPredictor:
    """æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åŠ¹æœäºˆæ¸¬"""
    
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100)
        self.is_trained = False
        
    def train(self, historical_data: pd.DataFrame):
        """éå»ã®æ”¹å–„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’"""
        features = [
            'current_cvr', 'bounce_rate', 'avg_time_on_page',
            'cta_position_y', 'mobile_ratio', 'page_speed'
        ]
        
        X = historical_data[features]
        y = historical_data['cvr_improvement']
        
        self.model.fit(X, y)
        self.is_trained = True
    
    def predict_improvement(self, current_metrics: Dict) -> float:
        """æ”¹å–„åŠ¹æœã®äºˆæ¸¬"""
        if not self.is_trained:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬
            return 0.15  # 15%æ”¹å–„
        
        X = pd.DataFrame([current_metrics])
        prediction = self.model.predict(X)[0]
        
        # ä¿¡é ¼åŒºé–“ã‚‚è¨ˆç®—
        trees_predictions = np.array([tree.predict(X) for tree in self.model.estimators_])
        confidence_lower = np.percentile(trees_predictions, 25)
        confidence_upper = np.percentile(trees_predictions, 75)
        
        return {
            'expected': prediction,
            'lower_bound': confidence_lower,
            'upper_bound': confidence_upper
        }
```

## 4. è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³

```python
# report_generator.py
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Table, Paragraph, Spacer, Image
from reportlab.lib.styles import getSampleStyleSheet
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64

class ReportGenerator:
    """è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    def __init__(self):
        self.styles = getSampleStyleSheet()
        sns.set_style("whitegrid")
    
    async def generate_weekly_report(self, site_id: str, data: Dict) -> bytes:
        """é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        # PDFãƒãƒƒãƒ•ã‚¡
        buffer = BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4)
        story = []
        
        # 1. ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
        story.append(Paragraph("é€±æ¬¡LPæ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ", self.styles['Title']))
        story.append(Spacer(1, 12))
        
        summary = f"""
        <b>æœŸé–“:</b> {data['date_range']}<br/>
        <b>CVRå¤‰åŒ–:</b> {data['cvr_change']:+.1%}<br/>
        <b>é‡è¦ãªç™ºè¦‹:</b><br/>
        """
        for insight in data['key_insights'][:3]:
            summary += f"â€¢ {insight}<br/>"
        
        story.append(Paragraph(summary, self.styles['Normal']))
        story.append(Spacer(1, 12))
        
        # 2. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”»åƒ
        heatmap_img = self.create_heatmap_visualization(data['heatmap_data'])
        story.append(Image(heatmap_img, width=400, height=300))
        story.append(Spacer(1, 12))
        
        # 3. æ”¹å–„ææ¡ˆãƒ†ãƒ¼ãƒ–ãƒ«
        improvements_data = [['å„ªå…ˆåº¦', 'æ”¹å–„å†…å®¹', 'æœŸå¾…åŠ¹æœ', 'å®Ÿè£…é›£æ˜“åº¦']]
        for idx, suggestion in enumerate(data['suggestions'][:5], 1):
            improvements_data.append([
                str(idx),
                suggestion['description'][:50],
                f"{suggestion['expected_impact']:+.0%}",
                suggestion['difficulty']
            ])
        
        improvements_table = Table(improvements_data)
        story.append(improvements_table)
        
        # 4. æ¬¡é€±ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        story.append(Paragraph("æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³", self.styles['Heading2']))
        actions = "<br/>".join([f"â˜ {action}" for action in data['next_actions']])
        story.append(Paragraph(actions, self.styles['Normal']))
        
        # PDFç”Ÿæˆ
        doc.build(story)
        pdf_bytes = buffer.getvalue()
        buffer.close()
        
        return pdf_bytes
    
    def create_heatmap_visualization(self, heatmap_data: Dict) -> str:
        """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å¯è¦–åŒ–"""
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # ã‚¯ãƒªãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—åŒ–
        sns.heatmap(
            heatmap_data['click_matrix'],
            cmap='YlOrRd',
            cbar_kws={'label': 'ã‚¯ãƒªãƒƒã‚¯æ•°'},
            ax=ax
        )
        
        ax.set_title('ã‚¯ãƒªãƒƒã‚¯ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—')
        ax.set_xlabel('Xåº§æ¨™')
        ax.set_ylabel('Yåº§æ¨™')
        
        # ç”»åƒã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')
        buffer.seek(0)
        img_base64 = base64.b64encode(buffer.read()).decode()
        plt.close()
        
        return f"data:image/png;base64,{img_base64}"
    
    async def send_report_email(self, recipient: str, report_pdf: bytes):
        """ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ¡ãƒ¼ãƒ«é€ä¿¡"""
        # SendGrid/AWS SESç­‰ã§å®Ÿè£…
        pass
```

## 5. ã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆï¼ˆã‚³ã‚¹ãƒˆæœ€é©åŒ–ï¼‰

### â˜ï¸ AWSæ§‹æˆ

```yaml
# infrastructure/terraform/main.tf
provider "aws" {
  region = "ap-northeast-1"  # æ±äº¬ãƒªãƒ¼ã‚¸ãƒ§ãƒ³
}

# VPCè¨­å®š
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"
  enable_dns_hostnames = true
  
  tags = {
    Name = "kaizen-lp-vpc"
  }
}

# ECS Fargateï¼ˆAPIï¼‰
resource "aws_ecs_cluster" "api_cluster" {
  name = "kaizen-lp-api"
  
  setting {
    name  = "containerInsights"
    value = "enabled"
  }
}

resource "aws_ecs_service" "api_service" {
  name            = "kaizen-api"
  cluster         = aws_ecs_cluster.api_cluster.id
  task_definition = aws_ecs_task_definition.api_task.arn
  desired_count   = 2  # æœ€å°æ§‹æˆ
  launch_type     = "FARGATE"
  
  # ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è¨­å®š
  deployment_configuration {
    maximum_percent         = 200
    minimum_healthy_percent = 100
  }
}

# RDSï¼ˆPostgreSQLï¼‰
resource "aws_db_instance" "main" {
  identifier     = "kaizen-lp-db"
  engine         = "postgres"
  engine_version = "15.3"
  instance_class = "db.t3.micro"  # é–‹å§‹æ™‚ã¯æœ€å°
  
  allocated_storage     = 20
  max_allocated_storage = 100  # è‡ªå‹•æ‹¡å¼µ
  storage_encrypted     = true
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  skip_final_snapshot = false
}

# ElastiCacheï¼ˆRedisï¼‰
resource "aws_elasticache_cluster" "redis" {
  cluster_id           = "kaizen-cache"
  engine              = "redis"
  node_type           = "cache.t3.micro"  # æœ€å°æ§‹æˆ
  num_cache_nodes     = 1
  parameter_group_name = "default.redis7"
  port                = 6379
}

# CloudFrontï¼ˆCDNï¼‰
resource "aws_cloudfront_distribution" "cdn" {
  enabled             = true
  is_ipv6_enabled    = true
  default_root_object = "tracker.js"
  
  origin {
    domain_name = aws_s3_bucket.static.bucket_regional_domain_name
    origin_id   = "S3-static"
  }
  
  default_cache_behavior {
    allowed_methods  = ["GET", "HEAD"]
    cached_methods   = ["GET", "HEAD"]
    target_origin_id = "S3-static"
    
    forwarded_values {
      query_string = false
      cookies {
        forward = "none"
      }
    }
    
    viewer_protocol_policy = "redirect-to-https"
    min_ttl                = 0
    default_ttl            = 86400
    max_ttl                = 31536000
  }
  
  price_class = "PriceClass_200"  # æ—¥æœ¬ãƒ»ã‚¢ã‚¸ã‚¢æœ€é©åŒ–
  
  restrictions {
    geo_restriction {
      restriction_type = "none"
    }
  }
}

# Lambdaï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
resource "aws_lambda_function" "batch_processor" {
  filename      = "batch_processor.zip"
  function_name = "kaizen-batch-processor"
  role          = aws_iam_role.lambda_role.arn
  handler       = "index.handler"
  runtime       = "python3.11"
  timeout       = 300
  memory_size   = 512  # æœ€å°ã€œä¸­è¦æ¨¡
  
  environment {
    variables = {
      CLICKHOUSE_HOST = var.clickhouse_host
      REDIS_HOST      = aws_elasticache_cluster.redis.cache_nodes[0].address
    }
  }
}

# ã‚³ã‚¹ãƒˆæœ€é©åŒ–è¨­å®š
resource "aws_autoscaling_target" "ecs_target" {
  max_capacity       = 10
  min_capacity       = 1  # æœ€å°1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
  resource_id        = "service/${aws_ecs_cluster.api_cluster.name}/${aws_ecs_service.api_service.name}"
  scalable_dimension = "ecs:service:DesiredCount"
  service_namespace  = "ecs"
}

# CPUä½¿ç”¨ç‡ã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
resource "aws_autoscaling_policy" "cpu" {
  name               = "cpu-scaling"
  scaling_target_id  = aws_autoscaling_target.ecs_target.id
  policy_type        = "TargetTrackingScaling"
  
  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ECSServiceAverageCPUUtilization"
    }
    target_value = 70.0  # CPU 70%ã§ã‚¹ã‚±ãƒ¼ãƒ«
  }
}
```

## 6. æœˆé–“ã‚³ã‚¹ãƒˆè©¦ç®—ï¼ˆç¾å®Ÿçš„ï¼‰

```yaml
AWSæœˆé–“ã‚³ã‚¹ãƒˆï¼ˆ100ç¤¾åˆ©ç”¨æ™‚ï¼‰:
  Fargate:
    - 2 tasks Ã— 0.25 vCPU Ã— $0.05/hour = $36/æœˆ
    - 2 tasks Ã— 0.5 GB Ã— $0.005/hour = $7.2/æœˆ
    
  RDS (PostgreSQL):
    - db.t3.micro: $15/æœˆ
    - Storage 20GB: $2.3/æœˆ
    - Backup: $1/æœˆ
    
  ElastiCache (Redis):
    - cache.t3.micro: $13/æœˆ
    
  CloudFront:
    - è»¢é€é‡ 100GB: $14/æœˆ
    - ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: $2/æœˆ
    
  Lambda:
    - 100ä¸‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: $2/æœˆ
    - å®Ÿè¡Œæ™‚é–“: $5/æœˆ
    
  S3:
    - Storage 50GB: $1.15/æœˆ
    - ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: $5/æœˆ
    
  ãã®ä»–:
    - CloudWatch: $10/æœˆ
    - Data Transfer: $10/æœˆ
    
  åˆè¨ˆ: ç´„$125/æœˆï¼ˆç´„19,000å††ï¼‰
  
1ç¤¾ã‚ãŸã‚Šã‚³ã‚¹ãƒˆ: 190å††/æœˆ

åˆ©ç›Šç‡:
  å£²ä¸Š: 9,800å††/ç¤¾
  AWS: 190å††/ç¤¾
  ãã®ä»–: 2,000å††/ç¤¾ï¼ˆäººä»¶è²»ç­‰ï¼‰
  åˆ©ç›Š: 7,610å††/ç¤¾ï¼ˆ77.6%ï¼‰
```

## ã¾ã¨ã‚

### âœ… å®Ÿè£…ã®ç¾å®Ÿæ€§
- **æŠ€è¡“çš„é›£æ˜“åº¦**: ä¸­ç¨‹åº¦ï¼ˆæ—¢å­˜æŠ€è¡“ã®çµ„ã¿åˆã‚ã›ï¼‰
- **é–‹ç™ºæœŸé–“**: 3-4ãƒ¶æœˆã§MVPå¯èƒ½
- **åˆæœŸæŠ•è³‡**: 1,000-1,500ä¸‡å††
- **åç›Šæ€§**: é«˜ã„ï¼ˆåˆ©ç›Šç‡70%ä»¥ä¸Šï¼‰
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: å„ªç§€ï¼ˆè‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰

ã“ã‚Œãªã‚‰ç¾å®Ÿçš„ã«å®Ÿè£…ãƒ»é‹ç”¨å¯èƒ½ã§ã™ã€‚